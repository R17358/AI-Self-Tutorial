# -*- coding: utf-8 -*-
"""sentiment-LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1irJqycWXnvNHxjEPz3ThTIwnPbbK-aUN
"""

from google.colab import files
files.upload()

import kagglehub

# Download latest version
path = kagglehub.dataset_download("columbine/imdb-dataset-sentiment-analysis-in-csv-format")

print("Path to dataset files:", path)

import os

# List files in the dataset directory
dataset_dir = path
files = os.listdir(dataset_dir)

print("Available files:", files)

data_path = os.path.join(dataset_dir, "Train.csv")

import pandas as pd
# Load the dataset
df = pd.read_csv(data_path)

# Check the first few rows of the dataframe to understand its structure
print(df.head())

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
import numpy as np

# Load dataset (Here, we use the IMDb dataset)
from tensorflow.keras.datasets import imdb

# Load the IMDb dataset
max_words = 10000  # Consider top 10000 words
max_len = 150      # Pad sequences to this length

# Load training and test data
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)

# Pad sequences to ensure they are of the same length
X_train_pad = pad_sequences(X_train, maxlen=max_len)
X_test_pad = pad_sequences(X_test, maxlen=max_len)

# Build the deep learning model (LSTM)
model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    LSTM(128, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_data=(X_test_pad, y_test))

# Evaluate the model on test data
test_loss, test_acc = model.evaluate(X_test_pad, y_test)
print(f"Test Accuracy: {test_acc}")

# Predict sentiment for new text
def predict_sentiment(text):
    # Tokenize and pad the new text
    tokenizer = Tokenizer(num_words=max_words)
    tokenizer.fit_on_texts([text])
    seq = tokenizer.texts_to_sequences([text])
    padded_seq = pad_sequences(seq, maxlen=max_len)

    # Predict sentiment using the trained model
    prediction = model.predict(padded_seq)

    if prediction >= 0.5:
        return "Positive"
    else:
        return "Negative"

# Example usage of prediction
new_text = "I love this movie! It's so amazing."
print(f"Sentiment: {predict_sentiment(new_text)}")

