import librosa
import numpy as np

def extract_mfcc(audio_path, sr=22050, n_mfcc=13):
    audio, _ = librosa.load(audio_path, sr=sr)
    mfcc = librosa.feature.mfcc(audio, sr=sr, n_mfcc=n_mfcc)
    return mfcc

source_mfcc = extract_mfcc("source_voice.wav")
target_mfcc = extract_mfcc("target_voice.wav")


import tensorflow as tf
from tensorflow.keras import layers

# Example Generator for CycleGAN
def build_generator():
    model = tf.keras.Sequential([
        layers.Dense(256, input_dim=100),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(),
        layers.Dense(512),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(),
        layers.Dense(1024),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(),
        layers.Dense(64*64*1, activation='tanh'),
        layers.Reshape((64, 64, 1))  # Output size
    ])
    return model

# Example Discriminator for CycleGAN
def build_discriminator():
    model = tf.keras.Sequential([
        layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=(64, 64, 1)),
        layers.LeakyReLU(0.2),
        layers.Conv2D(128, kernel_size=3, strides=2, padding='same'),
        layers.LeakyReLU(0.2),
        layers.Flatten(),
        layers.Dense(1, activation='sigmoid')  # Output: Real/Fake
    ])
    return model


# Define loss functions
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    return real_loss + fake_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

# Training loop (simplified)
epochs = 10000
for epoch in range(epochs):
    # Train Discriminator (real vs fake target voice)
    with tf.GradientTape() as disc_tape:
        real_output = discriminator(target_mfcc)  # Real target voice
        fake_output = discriminator(generator(source_mfcc))  # Fake target voice
        disc_loss = discriminator_loss(real_output, fake_output)
    grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    discriminator_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))

    # Train Generator (try to fool the Discriminator)
    with tf.GradientTape() as gen_tape:
        fake_output = discriminator(generator(source_mfcc))
        gen_loss = generator_loss(fake_output)
    grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)
    generator_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))

